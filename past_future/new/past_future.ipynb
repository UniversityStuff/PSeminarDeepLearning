{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XATRMCTF8wMC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "from rasterio.enums import Resampling\n",
        "from shapely.geometry import box\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import glob\n",
        "import math\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Verbinde Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "temp_dir = os.path.join(base_dir, 'Temp_Processing')\n",
        "\n",
        "for d in [data_dir, map_dir, temp_dir]:\n",
        "    if not os.path.exists(d): os.makedirs(d)\n",
        "\n",
        "train_file = os.path.join(data_dir, 'landslides.csv')\n",
        "\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Muncipalities_polygon.shp'\n",
        "\n",
        "# CRU TS Links\n",
        "cru_urls = {\n",
        "    \"tmin_2010\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_tmin_2010-2019.zip\",\n",
        "    \"tmin_2020\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_tmin_2020-2024.zip\",\n",
        "    \"tmax_2010\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_tmax_2010-2019.zip\",\n",
        "    \"tmax_2020\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_tmax_2020-2024.zip\",\n",
        "    \"prec_2010\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_prec_2010-2019.zip\",\n",
        "    \"prec_2020\": \"https://geodata.ucdavis.edu/climate/worldclim/2_1/hist/cts4.09/wc2.1_cruts4.09_2.5m_prec_2020-2024.zip\"\n",
        "}\n",
        "\n",
        "features = [\n",
        "    'Elevation', 'Slope', 'Aspect',\n",
        "    'BIO01_Historical_Mean', 'BIO05_Historical_Max', 'BIO06_Historical_Min',\n",
        "    'BIO12_Historical_Prec', 'BIO13_Historical_Prec', 'BIO15_Historical_Prec'\n",
        "]\n",
        "\n",
        "# Grid Setup (ca 500m)\n",
        "GRID_RES = 0.00416\n",
        "BBOX = (10.0, 46.0, 13.0, 47.5)\n",
        "geo_bbox = gpd.GeoDataFrame({'geometry': box(*BBOX)}, index=[0], crs=\"EPSG:4326\")\n",
        "\n",
        "# TEIL 1: MODELL TRAINING\n",
        "print(\"\\n 1. MODELL TRAINING\")\n",
        "if not os.path.exists(train_file):\n",
        "    print(f\"FEHLER: Lade '{os.path.basename(train_file)}' nach '{data_dir}'!\")\n",
        "    exit()\n",
        "\n",
        "df_train = pd.read_csv(train_file)\n",
        "# Datum suchen für Validierung\n",
        "date_col = None\n",
        "for col in df_train.columns:\n",
        "    if any(x in col.lower() for x in ['date', 'jahr', 'year']):\n",
        "        date_col = col; break\n",
        "print(f\"Datumsspalte: {date_col}\")\n",
        "\n",
        "# Features vorbereiten\n",
        "for f in features:\n",
        "    if f not in df_train.columns: df_train[f] = 0\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
        "                            class_weight='balanced', max_features='sqrt', n_jobs=-1, random_state=42)\n",
        "rf.fit(df_train[features].fillna(0), df_train['Target'])\n",
        "print(\"Modell trainiert.\")\n",
        "\n",
        "# TEIL 2: GITTER & TOPO\n",
        "print(\"\\n2. GITTER & TOPOGRAPHIE\")\n",
        "grid_csv = os.path.join(temp_dir, 'Grid_Topo_Only.csv')\n",
        "\n",
        "if not os.path.exists(grid_csv):\n",
        "    # Gitter erstellen\n",
        "    lon = np.arange(BBOX[0], BBOX[2], GRID_RES)\n",
        "    lat = np.arange(BBOX[1], BBOX[3], GRID_RES)\n",
        "    xx, yy = np.meshgrid(lon, lat)\n",
        "    df_grid = pd.DataFrame({'Longitude': xx.flatten(), 'Latitude': yy.flatten()})\n",
        "    print(f\"Gitter: {len(df_grid)} Punkte\")\n",
        "\n",
        "    # Elevation Download\n",
        "    elev_zip = os.path.join(temp_dir, \"elev.zip\")\n",
        "    elev_tif = os.path.join(temp_dir, \"wc2.1_30s_elev.tif\")\n",
        "\n",
        "    if not os.path.exists(elev_tif):\n",
        "        print(\"Lade DEM...\")\n",
        "        with requests.get(\"https://geodata.ucdavis.edu/climate/worldclim/2_1/base/wc2.1_30s_elev.zip\", stream=True) as r:\n",
        "            with open(elev_zip, 'wb') as f: shutil.copyfileobj(r.raw, f)\n",
        "        with zipfile.ZipFile(elev_zip, 'r') as z:\n",
        "            z.extract([n for n in z.namelist() if n.endswith('.tif')][0], temp_dir)\n",
        "            os.rename(os.path.join(temp_dir, [n for n in z.namelist() if n.endswith('.tif')][0]), elev_tif)\n",
        "\n",
        "    # Slope/Aspect berechnen\n",
        "    print(\"Berechne Slope/Aspect...\")\n",
        "    with rasterio.open(elev_tif) as src:\n",
        "        coords = [(r.Longitude, r.Latitude) for _, r in df_grid.iterrows()]\n",
        "        df_grid['Elevation'] = [x[0] for x in src.sample(coords)]\n",
        "\n",
        "        # Mask für Gradienten\n",
        "        out_img, out_transform = mask(src, geo_bbox.geometry, crop=True)\n",
        "        data = out_img[0].astype('float32')\n",
        "        res_x, res_y = out_transform[0], -out_transform[4]\n",
        "        scale_x = 111320 * math.cos(46.5*math.pi/180) * res_x\n",
        "        scale_y = 111320 * res_y\n",
        "\n",
        "        dy, dx = np.gradient(data)\n",
        "        slope = np.degrees(np.arctan(np.sqrt((dx/scale_x)**2 + (dy/scale_y)**2)))\n",
        "        aspect = (np.degrees(np.arctan2(dy, -dx)) + 360) % 360\n",
        "\n",
        "        # In Temp-Tifs schreiben für Sampling\n",
        "        meta = src.meta.copy()\n",
        "        meta.update({\"height\": data.shape[0], \"width\": data.shape[1], \"transform\": out_transform})\n",
        "        with rasterio.open(os.path.join(temp_dir, 'slope.tif'), 'w', **meta) as dst: dst.write(slope, 1)\n",
        "        with rasterio.open(os.path.join(temp_dir, 'aspect.tif'), 'w', **meta) as dst: dst.write(aspect, 1)\n",
        "\n",
        "    with rasterio.open(os.path.join(temp_dir, 'slope.tif')) as s:\n",
        "        df_grid['Slope'] = [x[0] for x in s.sample(coords)]\n",
        "    with rasterio.open(os.path.join(temp_dir, 'aspect.tif')) as a:\n",
        "        df_grid['Aspect'] = [x[0] for x in a.sample(coords)]\n",
        "\n",
        "    df_grid.to_csv(grid_csv, index=False)\n",
        "    print(\"Topo fertig.\")\n",
        "else:\n",
        "    print(\"Lade existierendes Topo-Grid...\")\n",
        "    df_grid = pd.read_csv(grid_csv)\n",
        "\n",
        "\n",
        "# TEIL 3: DATEN (2015-2024)\n",
        "print(\"\\n3. DATEN VERARBEITUNG (2015-2024)\")\n",
        "# Hilfsfunktion (angepasst an Grid)\n",
        "def process_cru_downloads():\n",
        "    all_data = {'tmin': {}, 'tmax': {}, 'prec': {}}\n",
        "\n",
        "    for key, url in cru_urls.items():\n",
        "        var_type = key.split('_')[0]\n",
        "        zip_path = os.path.join(temp_dir, f\"{key}.zip\")\n",
        "        extract_path = os.path.join(temp_dir, key)\n",
        "\n",
        "        # Download\n",
        "        if not os.path.exists(extract_path):\n",
        "            print(f\"Lade {key}...\")\n",
        "            if not os.path.exists(zip_path):\n",
        "                with requests.get(url, stream=True) as r:\n",
        "                    with open(zip_path, 'wb') as f: shutil.copyfileobj(r.raw, f)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as z: z.extractall(extract_path)\n",
        "            os.remove(zip_path)\n",
        "\n",
        "        # Lesen & Croppen\n",
        "        tifs = glob.glob(f\"{extract_path}/*.tif\")\n",
        "        print(f\"  Verarbeite {len(tifs)} Dateien für {key}...\")\n",
        "\n",
        "        for tif in tifs:\n",
        "            try:\n",
        "                # Dateiname parsen (manchmal '..._2010-01.tif')\n",
        "                date_part = os.path.basename(tif).replace('.tif','').split('_')[-1]\n",
        "                y, m = map(int, date_part.split('-'))\n",
        "\n",
        "                if 2015 <= y <= 2024:\n",
        "                    with rasterio.open(tif) as src:\n",
        "                        out_img, out_trans = mask(src, geo_bbox.geometry, crop=True)\n",
        "                        all_data[var_type][f\"{m:02d}_{y}\"] = (out_img[0], out_trans, src.crs, src.meta)\n",
        "            except: continue\n",
        "\n",
        "        shutil.rmtree(extract_path) # Aufräumen\n",
        "    return all_data\n",
        "\n",
        "# Daten laden\n",
        "cru_data = process_cru_downloads()\n",
        "print(\"Rohdaten im Speicher.\")\n",
        "\n",
        "# Klima berechnen\n",
        "print(\"Berechne Durchschnitte & BIOs...\")\n",
        "bio_values = {}\n",
        "# Referenz-Meta für das Upscaling später\n",
        "ref_meta = None\n",
        "\n",
        "# Mittelwerte berechnen\n",
        "monthly_means = {'tmin': [], 'tmax': [], 'prec': []}\n",
        "\n",
        "for month in range(1, 13):\n",
        "    m_str = f\"{month:02d}\"\n",
        "    for var in ['tmin', 'tmax', 'prec']:\n",
        "        arrays = []\n",
        "        for y in range(2015, 2025):\n",
        "            k = f\"{m_str}_{y}\"\n",
        "            if k in cru_data[var]:\n",
        "                data, trans, crs, meta = cru_data[var][k]\n",
        "                arrays.append(data)\n",
        "                if ref_meta is None: ref_meta = meta.copy(); ref_meta.update({\"height\": data.shape[0], \"width\": data.shape[1], \"transform\": trans})\n",
        "\n",
        "        if arrays:\n",
        "            monthly_means[var].append(np.mean(np.stack(arrays), axis=0))\n",
        "        else:\n",
        "            print(f\"WARNUNG: Keine Daten für {var} Monat {m}\")\n",
        "\n",
        "# Stapeln (12 Monate)\n",
        "tmin = np.stack(monthly_means['tmin'])\n",
        "tmax = np.stack(monthly_means['tmax'])\n",
        "prec = np.stack(monthly_means['prec'])\n",
        "\n",
        "# BIOs\n",
        "bio_values['BIO01_Historical_Mean'] = np.mean((tmax + tmin) / 2, axis=0)\n",
        "bio_values['BIO05_Historical_Max'] = np.max(tmax, axis=0)\n",
        "bio_values['BIO06_Historical_Min'] = np.min(tmin, axis=0)\n",
        "bio_values['BIO12_Historical_Prec'] = np.sum(prec, axis=0)\n",
        "bio_values['BIO13_Historical_Prec'] = np.max(prec, axis=0)\n",
        "prec_mean = np.mean(prec, axis=0)\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    bio15 = (np.std(prec, axis=0) / prec_mean) * 100\n",
        "    bio15[prec_mean == 0] = 0\n",
        "bio_values['BIO15_Historical_Prec'] = bio15\n",
        "\n",
        "print(\"BIOs berechnet. Starte Upscaling & Mapping auf Grid...\")\n",
        "# Upscaling und Zuweisung an Grid\n",
        "coords = [(r.Longitude, r.Latitude) for _, r in df_grid.iterrows()]\n",
        "\n",
        "for name, grid_data in bio_values.items():\n",
        "    # MemoryFile für das Upscaling\n",
        "    with rasterio.io.MemoryFile() as memfile:\n",
        "        with memfile.open(**ref_meta) as dataset:\n",
        "            dataset.write(grid_data.astype('float32'), 1)\n",
        "\n",
        "            # Upscaling (Faktor 10 für glatte ~500m)\n",
        "            upscale = 10\n",
        "            new_h = int(dataset.height * upscale)\n",
        "            new_w = int(dataset.width * upscale)\n",
        "\n",
        "            data_up = dataset.read(\n",
        "                1, out_shape=(1, new_h, new_w), resampling=Resampling.bilinear\n",
        "            )\n",
        "\n",
        "            # Neue Transformation berechnen\n",
        "            trans_up = dataset.transform * dataset.transform.scale(\n",
        "                (dataset.width / data_up.shape[-1]),\n",
        "                (dataset.height / data_up.shape[-2])\n",
        "            )\n",
        "\n",
        "            # Temporäres High-Res File im RAM\n",
        "            up_meta = dataset.meta.copy()\n",
        "            up_meta.update({\"height\": new_h, \"width\": new_w, \"transform\": trans_up})\n",
        "\n",
        "            with rasterio.io.MemoryFile() as up_mem:\n",
        "                with up_mem.open(**up_meta) as up_ds:\n",
        "                    up_ds.write(data_up, 1)\n",
        "                    # Samplen für die Gitterpunkte\n",
        "                    df_grid[name] = [x[0] for x in up_ds.sample(coords)]\n",
        "    print(f\"  -> {name} fertig.\")\n",
        "\n",
        "# Speichern\n",
        "df_grid.to_csv(os.path.join(map_dir, 'Grid_Full_Data_2015_2024.csv'), index=False)\n",
        "\n",
        "\n",
        "# TEIL 4: VORHERSAGE & VALIDIERUNG\n",
        "print(\"\\n 4. MAPPING & VALIDIERUNG\")\n",
        "\n",
        "# Vorhersage\n",
        "print(\"Berechne Risiko...\")\n",
        "probs = rf.predict_proba(df_grid[features].fillna(0))[:, 1]\n",
        "df_grid['Landslide_Probability'] = probs\n",
        "\n",
        "# Speichern\n",
        "df_grid.to_csv(os.path.join(map_dir, 'Risk_Map_Data_2015_2024.csv'), index=False)\n",
        "\n",
        "# Basis-Plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(df_grid.Longitude, df_grid.Latitude, c=df_grid.Landslide_Probability, cmap='Reds', s=2, marker='s', vmin=0, vmax=1)\n",
        "plt.colorbar(label='Risk Probability')\n",
        "plt.title('Historical Landslide Susceptibility (2015-2024)')\n",
        "plt.savefig(os.path.join(map_dir, 'Map_2015_2024.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Validierung (Jahres-Overlay)\n",
        "if date_col:\n",
        "    df_train['Year'] = pd.to_datetime(df_train[date_col], errors='coerce').dt.year\n",
        "    years = sorted(df_train[df_train.Target == 1]['Year'].dropna().unique().astype(int))\n",
        "\n",
        "    print(f\"Erstelle Validierungskarten für: {years}\")\n",
        "    for year in years:\n",
        "        if year < 2015: continue\n",
        "\n",
        "        events = df_train[(df_train.Target == 1) & (df_train.Year == year)]\n",
        "        if len(events) == 0: continue\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.scatter(df_grid.Longitude, df_grid.Latitude, c=df_grid.Landslide_Probability, cmap='Reds', s=2, marker='s', vmin=0, vmax=1, alpha=0.6)\n",
        "        plt.colorbar(label='Risk Probability')\n",
        "\n",
        "        # Events\n",
        "        plt.scatter(events.Longitude, events.Latitude, c='black', marker='x', s=100, linewidth=2, label=f'Events {year}')\n",
        "\n",
        "        plt.title(f\"Validation Year {year} (Events vs 2015-2024 Risk)\")\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(map_dir, f'Validation_Map_{year}.png'), dpi=300)\n",
        "        plt.close()\n",
        "        print(f\"  -> Jahr {year} fertig.\")\n",
        "\n",
        "# TEIL 5: GEMEINDE ANALYSE (Auf Basis der 2015-2024 Karte)\n",
        "print(\"\\n--- 5. GEMEINDE ANALYSE ---\")\n",
        "if os.path.exists(shapefile_path):\n",
        "    try:\n",
        "        print(\"Lade Shapefile...\")\n",
        "        gdf_gem = gpd.read_file(shapefile_path)\n",
        "\n",
        "        # CRS Check\n",
        "        if gdf_gem.crs != \"EPSG:4326\":\n",
        "            print(f\"Transformiere Shapefile von {gdf_gem.crs} nach EPSG:4326...\")\n",
        "            gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "\n",
        "        # Grid in GeoDataFrame umwandeln\n",
        "        print(\"Wandle Grid in Geometrie um...\")\n",
        "        gdf_points = gpd.GeoDataFrame(\n",
        "            df_grid,\n",
        "            geometry=gpd.points_from_xy(df_grid.Longitude, df_grid.Latitude),\n",
        "            crs=\"EPSG:4326\"\n",
        "        )\n",
        "\n",
        "        print(\"Spatial Join (Welcher Punkt liegt in welcher Gemeinde?)...\")\n",
        "        joined = gpd.sjoin(gdf_points, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "        # Namens-Spalte\n",
        "        name_col = 'NAME_DE'\n",
        "\n",
        "        print(f\"Gruppiere nach: {name_col}\")\n",
        "\n",
        "        # Aggregieren: Durchschnittliches Risiko pro Gemeinde\n",
        "        risk_gem = joined.groupby(name_col)['Landslide_Probability'].mean().reset_index()\n",
        "        risk_gem.columns = [name_col, 'Mean_Hazard']\n",
        "\n",
        "        # Zurück ins Shapefile mergen für die Karte\n",
        "        gdf_final = gdf_gem.merge(risk_gem, on=name_col)\n",
        "\n",
        "        # Plotten\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        ax = plt.gca()\n",
        "        gdf_final.plot(column='Mean_Hazard', ax=ax, cmap='OrRd', legend=True,\n",
        "                       legend_kwds={'label': \"Durchschnittl. Gefährdung (2015-2024 Climate)\"})\n",
        "        plt.title(\"Gemeinde-Gefährdungskarte\")\n",
        "        plt.savefig(os.path.join(map_dir, \"Municipality_Risk_Map.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "        # CSV Export\n",
        "        out_csv = os.path.join(map_dir, \"Municipality_Risk_Table.csv\")\n",
        "        gdf_final[[name_col, 'Mean_Hazard']].to_csv(out_csv, index=False)\n",
        "        print(f\"Gemeinde-Daten gespeichert: {out_csv}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler bei Gemeinde-Analyse: {e}\")\n",
        "else:\n",
        "    print(f\"Shapefile nicht gefunden unter: {shapefile_path}\")\n",
        "\n",
        "print(f\"\\nFERTIG! Alles gespeichert in: {map_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Drive verbinden\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Municipalities_polygon.shp'\n",
        "\n",
        "# Datei mit den berechneten Risiken (vom vorherigen Schritt)\n",
        "risk_csv = os.path.join(map_dir, 'Risk_Map_Data_2015_2024.csv')\n",
        "\n",
        "print(\"--- 5. GEMEINDE ANALYSE (REPARATUR) ---\")\n",
        "\n",
        "if not os.path.exists(risk_csv):\n",
        "    print(f\"FEHLER: Die Datei {risk_csv} wurde nicht gefunden. Lief Schritt 4 vorher durch?\")\n",
        "else:\n",
        "    print(f\"Lade Risikodaten: {os.path.basename(risk_csv)}...\")\n",
        "    df_grid = pd.read_csv(risk_csv)\n",
        "\n",
        "    if not os.path.exists(shapefile_path):\n",
        "        print(f\"FEHLER: Shapefile immer noch nicht gefunden unter:\\n{shapefile_path}\\nBitte Pfad prüfen!\")\n",
        "    else:\n",
        "        try:\n",
        "            print(\"Lade Shapefile...\")\n",
        "            gdf_gem = gpd.read_file(shapefile_path)\n",
        "\n",
        "            # CRS Check\n",
        "            if gdf_gem.crs != \"EPSG:4326\":\n",
        "                print(f\"Transformiere Shapefile von {gdf_gem.crs} nach EPSG:4326...\")\n",
        "                gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "\n",
        "            # Grid in GeoDataFrame umwandeln\n",
        "            print(\"Wandle Grid in Geometrie um...\")\n",
        "            gdf_points = gpd.GeoDataFrame(\n",
        "                df_grid,\n",
        "                geometry=gpd.points_from_xy(df_grid.Longitude, df_grid.Latitude),\n",
        "                crs=\"EPSG:4326\"\n",
        "            )\n",
        "\n",
        "            print(\"Spatial Join (Welcher Punkt liegt in welcher Gemeinde?)...\")\n",
        "            joined = gpd.sjoin(gdf_points, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "            # Namens-Spalte finden\n",
        "            name_col = 'NAME_DE'\n",
        "\n",
        "            print(f\"Gruppiere nach: {name_col}\")\n",
        "\n",
        "            # Aggregieren\n",
        "            risk_gem = joined.groupby(name_col)['Landslide_Probability'].mean().reset_index()\n",
        "            risk_gem.columns = [name_col, 'Mean_Hazard']\n",
        "\n",
        "            # Merge & Plot\n",
        "            gdf_final = gdf_gem.merge(risk_gem, on=name_col)\n",
        "\n",
        "            plt.figure(figsize=(12, 10))\n",
        "            ax = plt.gca()\n",
        "            gdf_final.plot(column='Mean_Hazard', ax=ax, cmap='OrRd', legend=True,\n",
        "                           legend_kwds={'label': \"Durchschnittl. Gefährdung (2015-2024)\"})\n",
        "            plt.title(\"Municipality Risk Map\")\n",
        "\n",
        "            out_png = os.path.join(map_dir, \"Municipality_Risk_Map.png\")\n",
        "            plt.savefig(out_png, dpi=300)\n",
        "            print(f\"Karte gespeichert: {out_png}\")\n",
        "\n",
        "            # CSV Export\n",
        "            out_csv = os.path.join(map_dir, \"Municipality_Risk_Table.csv\")\n",
        "            gdf_final[[name_col, 'Mean_Hazard']].to_csv(out_csv, index=False)\n",
        "            print(f\"Tabelle gespeichert: {out_csv}\")\n",
        "            print(\"\\nFERTIG! Jetzt hast du alles.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Fehler: {e}\")"
      ],
      "metadata": {
        "id": "H6xiH5rYZnZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "temp_dir = os.path.join(base_dir, 'Temp_Processing')\n",
        "\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Municipalities_polygon.shp'\n",
        "\n",
        "features = [\n",
        "    'Elevation', 'Slope', 'Aspect',\n",
        "    'BIO01_Historical_Mean', 'BIO05_Historical_Max', 'BIO06_Historical_Min',\n",
        "    'BIO12_Historical_Prec', 'BIO13_Historical_Prec', 'BIO15_Historical_Prec'\n",
        "]\n",
        "\n",
        "ssp_urls = {\n",
        "    'SSP126': \"https://geodata.ucdavis.edu/cmip6/30s/MPI-ESM1-2-HR/ssp126/wc2.1_30s_bioc_MPI-ESM1-2-HR_ssp126_2081-2100.tif\",\n",
        "    'SSP370': \"https://geodata.ucdavis.edu/cmip6/30s/MPI-ESM1-2-HR/ssp370/wc2.1_30s_bioc_MPI-ESM1-2-HR_ssp370_2081-2100.tif\",\n",
        "    'SSP585': \"https://geodata.ucdavis.edu/cmip6/30s/MPI-ESM1-2-HR/ssp585/wc2.1_30s_bioc_MPI-ESM1-2-HR_ssp585_2081-2100.tif\"\n",
        "}\n",
        "\n",
        "bio_map = {1:'BIO01_Historical_Mean', 5:'BIO05_Historical_Max', 6:'BIO06_Historical_Min',\n",
        "           12:'BIO12_Historical_Prec', 13:'BIO13_Historical_Prec', 15:'BIO15_Historical_Prec'}\n",
        "\n",
        "# 1. MODELL, GRID & SHAPEFILE LADEN\n",
        "print(\"Lade Daten & Shapefile...\")\n",
        "\n",
        "# Shapefile laden (für den Hintergrund)\n",
        "if os.path.exists(shapefile_path):\n",
        "    gdf_gem = gpd.read_file(shapefile_path)\n",
        "    if gdf_gem.crs != \"EPSG:4326\": gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "else:\n",
        "    print(\"ACHTUNG: Shapefile nicht gefunden! Karten werden ohne Grenzen erstellt.\")\n",
        "    gdf_gem = None\n",
        "\n",
        "# Grid laden\n",
        "grid_topo_path = os.path.join(temp_dir, 'Grid_Topo_Only.csv')\n",
        "if not os.path.exists(grid_topo_path):\n",
        "    print(\"FEHLER: 'Grid_Topo_Only.csv' fehlt. Bitte Teil 1 (CRU Skript) laufen lassen.\")\n",
        "    exit()\n",
        "\n",
        "df_grid_base = pd.read_csv(grid_topo_path)\n",
        "\n",
        "# Modell trainieren\n",
        "train_file = os.path.join(data_dir, 'landslides.csv')\n",
        "df_train = pd.read_csv(train_file)\n",
        "for f in features:\n",
        "    if f not in df_train.columns: df_train[f] = 0\n",
        "\n",
        "print(\"Trainiere Modell...\")\n",
        "rf = RandomForestClassifier(n_estimators=500, max_depth=15, min_samples_leaf=2,\n",
        "                            class_weight='balanced', max_features='sqrt', n_jobs=-1, random_state=42)\n",
        "rf.fit(df_train[features].fillna(0), df_train['Target'])\n",
        "\n",
        "\n",
        "# 2. ZUKUNFTS-SZENARIEN (Prediction & Mapping)\n",
        "results = {}\n",
        "\n",
        "for name, url in ssp_urls.items():\n",
        "    print(f\"\\n--- Verarbeite {name} ---\")\n",
        "    df_scen = df_grid_base.copy()\n",
        "    coords = [(r.Longitude, r.Latitude) for _, r in df_scen.iterrows()]\n",
        "\n",
        "    print(\"Streame Klimadaten...\")\n",
        "    with rasterio.open(url) as src:\n",
        "        for b_idx, col in bio_map.items():\n",
        "            if b_idx <= src.count:\n",
        "                df_scen[col] = [x[0] for x in src.sample(coords, indexes=b_idx)]\n",
        "            else:\n",
        "                df_scen[col] = 0\n",
        "\n",
        "    print(\"Vorhersage...\")\n",
        "    probs = rf.predict_proba(df_scen[features].fillna(0))[:, 1]\n",
        "    df_scen['Landslide_Probability'] = probs\n",
        "\n",
        "    # Speichern CSV\n",
        "    df_scen.to_csv(os.path.join(map_dir, f'Result_Grid_{name}.csv'), index=False)\n",
        "\n",
        "    # PLOTTING MIT HINTERGRUND\n",
        "    print(f\"Erstelle Karte für {name}...\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # 1. Die Risikokarte (Scatter)\n",
        "    # s=12 und marker='s' sorgt dafür, dass die Quadrate sich berühren -> Keine Lücken!\n",
        "    sc = ax.scatter(df_scen.Longitude, df_scen.Latitude,\n",
        "                    c=df_scen.Landslide_Probability,\n",
        "                    cmap='Reds', vmin=0, vmax=1,\n",
        "                    s=12, marker='s', zorder=1)\n",
        "\n",
        "    # 2. Gemeindegrenzen darüberlegen (Overlay)\n",
        "    if gdf_gem is not None:\n",
        "        gdf_gem.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.3, alpha=0.6, zorder=2)\n",
        "\n",
        "    plt.colorbar(sc, label='Probability')\n",
        "    plt.title(f'Future Projection: {name} (2081-2100)')\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "\n",
        "    out_png = os.path.join(map_dir, f'Map_{name}.png')\n",
        "    plt.savefig(out_png, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Für SSP3 speichern wir das DF für die Gemeinde-Analyse\n",
        "    if name == 'SSP370':\n",
        "        results['SSP3'] = df_scen\n",
        "\n",
        "\n",
        "# 3. GEMEINDE ANALYSE (Choropleth Map für SSP3)\n",
        "print(\"\\n--- GEMEINDE ANALYSE (SSP3) ---\")\n",
        "if 'SSP3' in results and gdf_gem is not None:\n",
        "    try:\n",
        "        df_ssp3 = results['SSP3']\n",
        "        gdf_points = gpd.GeoDataFrame(df_ssp3, geometry=gpd.points_from_xy(df_ssp3.Longitude, df_ssp3.Latitude), crs=\"EPSG:4326\")\n",
        "\n",
        "        print(\"Spatial Join...\")\n",
        "        joined = gpd.sjoin(gdf_points, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "        # Name finden\n",
        "        name_col = 'NAME_DE'\n",
        "        if name_col not in joined.columns:\n",
        "            cands = [c for c in joined.columns if 'NAME' in c]\n",
        "            name_col = cands[0] if cands else joined.columns[0]\n",
        "\n",
        "        print(f\"Aggregiere nach {name_col}...\")\n",
        "        risk_gem = joined.groupby(name_col)['Landslide_Probability'].mean().reset_index()\n",
        "        risk_gem.columns = [name_col, 'Mean_Hazard_SSP3']\n",
        "\n",
        "        gdf_final = gdf_gem.merge(risk_gem, on=name_col)\n",
        "\n",
        "        # Plot (Hier sind die Gemeinden selbst eingefärbt)\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "        gdf_final.plot(column='Mean_Hazard_SSP3', ax=ax, cmap='OrRd',\n",
        "                       legend=True, legend_kwds={'label': \"Durchschnittl. Gefährdung (SSP3)\"},\n",
        "                       edgecolor='black', linewidth=0.2)\n",
        "\n",
        "        plt.title(\"Gefährdung pro Gemeinde (Szenario SSP3-7.0)\")\n",
        "        plt.savefig(os.path.join(map_dir, \"Municipality_Risk_SSP3.png\"), dpi=300)\n",
        "\n",
        "        gdf_final[[name_col, 'Mean_Hazard_SSP3']].to_csv(os.path.join(map_dir, \"Municipality_SSP3_Table.csv\"), index=False)\n",
        "        print(\"Gemeinde SSP3 Karte fertig.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fehler Gemeinde Analyse: {e}\")\n",
        "\n",
        "print(\"\\nFERTIG!\")"
      ],
      "metadata": {
        "id": "X_f6eykQcW7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Municipalities_polygon.shp'\n",
        "\n",
        "# Szenarien-Namen\n",
        "scenarios = ['SSP126', 'SSP370', 'SSP585']\n",
        "\n",
        "# DATEN LADEN\n",
        "print(\"Lade Shapefile für den perfekten Zoom...\")\n",
        "if os.path.exists(shapefile_path):\n",
        "    gdf_gem = gpd.read_file(shapefile_path)\n",
        "    if gdf_gem.crs != \"EPSG:4326\": gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "\n",
        "    minx, miny, maxx, maxy = gdf_gem.total_bounds\n",
        "    # kleiner Puffer (0.05 Grad ~ 5km)\n",
        "    zoom_xlim = (minx - 0.05, maxx + 0.05)\n",
        "    zoom_ylim = (miny - 0.05, maxy + 0.05)\n",
        "    print(f\"Zoom gesetzt auf: Longitude {zoom_xlim}, Latitude {zoom_ylim}\")\n",
        "else:\n",
        "    print(\"Shapefile nicht gefunden! Kann nicht automatisch zoomen.\")\n",
        "    # Fallback\n",
        "    zoom_xlim = (10.3, 12.5)\n",
        "    zoom_ylim = (46.2, 47.1)\n",
        "\n",
        "\n",
        "# KARTEN NEU PLOTTEN\n",
        "for name in scenarios:\n",
        "    csv_path = os.path.join(map_dir, f'Result_Grid_{name}.csv')\n",
        "\n",
        "    if os.path.exists(csv_path):\n",
        "        print(f\"Erstelle optimierte Karte für {name}...\")\n",
        "        df_scen = pd.read_csv(csv_path)\n",
        "\n",
        "        # Plotting Setup\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        # 1. Pixel-Map (Grün -> Rot)\n",
        "        # (Nur Punkte innerhalb des Zooms plotten)\n",
        "        mask = (df_scen.Longitude >= zoom_xlim[0]) & (df_scen.Longitude <= zoom_xlim[1]) & \\\n",
        "               (df_scen.Latitude >= zoom_ylim[0]) & (df_scen.Latitude <= zoom_ylim[1])\n",
        "        df_plot = df_scen[mask]\n",
        "\n",
        "        sc = ax.scatter(df_plot.Longitude, df_plot.Latitude,\n",
        "                        c=df_plot.Landslide_Probability,\n",
        "                        cmap='RdYlGn_r', vmin=0, vmax=1,\n",
        "                        s=15, marker='s', zorder=1, alpha=0.70)\n",
        "\n",
        "        # 2. Shapefile Overlay\n",
        "        if gdf_gem is not None:\n",
        "            gdf_gem.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.4, alpha=0.9, zorder=2)\n",
        "\n",
        "        # 3. Den Zoom anwenden\n",
        "        ax.set_xlim(zoom_xlim)\n",
        "        ax.set_ylim(zoom_ylim)\n",
        "\n",
        "        plt.colorbar(sc, label='Probability (0=Safe, 1=High Risk)')\n",
        "        plt.title(f'Future Projection: {name} (2081-2100)')\n",
        "        plt.xlabel('Longitude')\n",
        "        plt.ylabel('Latitude')\n",
        "\n",
        "        out_file = os.path.join(map_dir, f'Map_{name}_Zoomed.png')\n",
        "        plt.savefig(out_file, dpi=300, bbox_inches='tight') # bbox_inches='tight' entfernt weiße Ränder außen\n",
        "        plt.close()\n",
        "        print(f\"Gespeichert: {out_file}\")\n",
        "\n",
        "# GEMEINDE KARTE AUCH UPDATEN\n",
        "ssp3_table = os.path.join(map_dir, \"Municipality_SSP3_Table.csv\")\n",
        "if os.path.exists(ssp3_table) and gdf_gem is not None:\n",
        "    print(\"Erstelle optimierte Gemeindekarte...\")\n",
        "    df_gem_data = pd.read_csv(ssp3_table)\n",
        "\n",
        "    # Spaltenname finden (Join Key)\n",
        "    key_col = df_gem_data.columns[0]\n",
        "\n",
        "    # Merge\n",
        "    gdf_final = gdf_gem.merge(df_gem_data, on=key_col)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    gdf_final.plot(column='Mean_Hazard_SSP3', ax=ax,\n",
        "                   cmap='RdYlGn_r',\n",
        "                   legend=True, legend_kwds={'label': \"Durchschnittl. Gefährdung\"},\n",
        "                   edgecolor='black', linewidth=0.3)\n",
        "\n",
        "    ax.set_xlim(zoom_xlim)\n",
        "    ax.set_ylim(zoom_ylim)\n",
        "    plt.title(\"Gefährdung pro Gemeinde (SSP3, Zoomed)\")\n",
        "    plt.savefig(os.path.join(map_dir, \"Municipality_Risk_SSP3_Zoomed.png\"), dpi=300, bbox_inches='tight')\n",
        "    print(\"Gemeindekarte gespeichert.\")\n",
        "\n",
        "print(\"\\nFERTIG!\")"
      ],
      "metadata": {
        "id": "b1gP6Gmff__z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Municipalities_polygon.shp'\n",
        "hist_csv_path = os.path.join(map_dir, 'Risk_Map_Data_2015_2024.csv')\n",
        "\n",
        "# DATEN LADEN & ZOOM BERECHNEN\n",
        "print(\"Lade Shapefile...\")\n",
        "if os.path.exists(shapefile_path):\n",
        "    gdf_gem = gpd.read_file(shapefile_path)\n",
        "    if gdf_gem.crs != \"EPSG:4326\": gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "\n",
        "    # Auto-Zoom\n",
        "    minx, miny, maxx, maxy = gdf_gem.total_bounds\n",
        "    zoom_xlim = (minx - 0.02, maxx + 0.02)\n",
        "    zoom_ylim = (miny - 0.02, maxy + 0.02)\n",
        "    print(f\"Zoom: {zoom_xlim}, {zoom_ylim}\")\n",
        "else:\n",
        "    print(\"Shapefile nicht gefunden!\")\n",
        "    exit()\n",
        "\n",
        "# 1. HISTORISCHE PIXEL-KARTE (2015-2024)\n",
        "if os.path.exists(hist_csv_path):\n",
        "    print(\"Erstelle Historische Karte (Pixel)...\")\n",
        "    df_hist = pd.read_csv(hist_csv_path)\n",
        "\n",
        "    # Filter für schnelleres Plotten (Zoom)\n",
        "    mask = (df_hist.Longitude >= zoom_xlim[0]) & (df_hist.Longitude <= zoom_xlim[1]) & \\\n",
        "           (df_hist.Latitude >= zoom_ylim[0]) & (df_hist.Latitude <= zoom_ylim[1])\n",
        "    df_plot = df_hist[mask]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # Pixel (Grün -> Rot, weich)\n",
        "    sc = ax.scatter(df_plot.Longitude, df_plot.Latitude,\n",
        "                    c=df_plot.Landslide_Probability,\n",
        "                    cmap='RdYlGn_r', vmin=0, vmax=1,\n",
        "                    s=15, marker='s', zorder=1, alpha=0.7)\n",
        "\n",
        "    # Shapefile Overlay (Stark)\n",
        "    gdf_gem.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.5, alpha=0.9, zorder=2)\n",
        "\n",
        "    # Zoom\n",
        "    ax.set_xlim(zoom_xlim)\n",
        "    ax.set_ylim(zoom_ylim)\n",
        "\n",
        "    plt.colorbar(sc, label='Probability (0=Safe, 1=High Risk)')\n",
        "    plt.title('Historical Susceptibility (2015-2024)')\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "\n",
        "    out_file = os.path.join(map_dir, 'Map_Historical_2015_2024_Zoomed.png')\n",
        "    plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Gespeichert: {out_file}\")\n",
        "\n",
        "# 2. GEMEINDE-KARTE (2015-2024)\n",
        "print(\"Erstelle Historische Gemeinde-Karte...\")\n",
        "\n",
        "# Spatial Join (Punkte -> Gemeinden)\n",
        "gdf_points = gpd.GeoDataFrame(\n",
        "    df_hist,\n",
        "    geometry=gpd.points_from_xy(df_hist.Longitude, df_hist.Latitude),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# Join\n",
        "joined = gpd.sjoin(gdf_points, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "# Name finden\n",
        "name_col = 'NAME_DE'\n",
        "if name_col not in joined.columns:\n",
        "    cands = [c for c in joined.columns if 'NAME' in c]\n",
        "    name_col = cands[0] if cands else joined.columns[0]\n",
        "\n",
        "# Aggregieren\n",
        "risk_gem = joined.groupby(name_col)['Landslide_Probability'].mean().reset_index()\n",
        "risk_gem.columns = [name_col, 'Mean_Hazard_Hist']\n",
        "\n",
        "# Merge zurück ins Shapefile\n",
        "gdf_final = gdf_gem.merge(risk_gem, on=name_col)\n",
        "\n",
        "# Plot (Grün -> Rot)\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "gdf_final.plot(column='Mean_Hazard_Hist', ax=ax,\n",
        "               cmap='RdYlGn_r', alpha=0.8,\n",
        "               legend=True, legend_kwds={'label': \"Durchschnittl. Gefährdung (2015-2024)\"},\n",
        "               edgecolor='black', linewidth=0.3)\n",
        "\n",
        "ax.set_xlim(zoom_xlim)\n",
        "ax.set_ylim(zoom_ylim)\n",
        "plt.title(\"Gefährdung pro Gemeinde (2015-2024)\")\n",
        "\n",
        "out_gem = os.path.join(map_dir, \"Municipality_Risk_2015_2024_Zoomed.png\")\n",
        "plt.savefig(out_gem, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# CSV Speichern\n",
        "gdf_final[[name_col, 'Mean_Hazard_Hist']].to_csv(os.path.join(map_dir, \"Municipality_Risk_2015_2024_Table.csv\"), index=False)\n",
        "print(f\"Gespeichert: {out_gem}\")\n",
        "\n",
        "print(\"\\nFERTIG!\")"
      ],
      "metadata": {
        "id": "Z4qcFXjljCEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "\n",
        "features = [\n",
        "    'Elevation', 'Slope', 'Aspect',\n",
        "    'BIO01_Historical_Mean', 'BIO05_Historical_Max', 'BIO06_Historical_Min',\n",
        "    'BIO12_Historical_Prec', 'BIO13_Historical_Prec', 'BIO15_Historical_Prec'\n",
        "]\n",
        "\n",
        "# 2. DATEN LADEN & SPLIT\n",
        "print(\"Lade Daten...\")\n",
        "train_file = os.path.join(data_dir, 'landslides.csv')\n",
        "df = pd.read_csv(train_file)\n",
        "for f in features:\n",
        "    if f not in df.columns: df[f] = 0\n",
        "\n",
        "X = df[features].fillna(0)\n",
        "y = df['Target']\n",
        "\n",
        "# Split (Konsistent mit Training)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 3. MODELL TRAINIEREN\n",
        "print(\"Trainiere Random Forest...\")\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=15,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight='balanced',\n",
        "    max_features='sqrt',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# --- TEIL A: OVERFITTING CHECK ---\n",
        "print(\"\\n--- A. OVERFITTING CHECK ---\")\n",
        "y_pred_train = rf.predict(X_train)\n",
        "y_pred_test = rf.predict(X_test)\n",
        "\n",
        "acc_train = accuracy_score(y_train, y_pred_train)\n",
        "acc_test = accuracy_score(y_test, y_pred_test)\n",
        "f1_train = f1_score(y_train, y_pred_train)\n",
        "f1_test = f1_score(y_test, y_pred_test)\n",
        "\n",
        "gap = acc_train - acc_test\n",
        "print(f\"Training Accuracy: {acc_train:.2%}\")\n",
        "print(f\"Test Accuracy:     {acc_test:.2%}\")\n",
        "print(f\"Gap (Overfitting): {gap:.2%}\")\n",
        "\n",
        "# Plot Overfitting\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(['Training', 'Test'], [acc_train, acc_test], color=['navy', 'orange'], alpha=0.7)\n",
        "plt.title('Overfitting Check (Accuracy)')\n",
        "plt.ylim(0, 1.1)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.savefig(os.path.join(map_dir, 'Stat_Overfitting_Chart.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --- TEIL B: DETAIL METRIKEN ---\n",
        "print(\"\\n--- B. TEST PERFORMANCE ---\")\n",
        "prec = precision_score(y_test, y_pred_test)\n",
        "rec = recall_score(y_test, y_pred_test)\n",
        "report = classification_report(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Precision: {prec:.2%}\")\n",
        "print(f\"Recall:    {rec:.2%}\")\n",
        "print(f\"F1-Score:  {f1_test:.2%}\")\n",
        "\n",
        "# Speichern als Text\n",
        "with open(os.path.join(map_dir, 'Stat_Full_Report.txt'), 'w') as f:\n",
        "    f.write(\"MODEL REPORT\\n\")\n",
        "    f.write(f\"1. OVERFITTING CHECK\\n\")\n",
        "    f.write(f\"Train Acc: {acc_train:.4f}\\nTest Acc:  {acc_test:.4f}\\nGap:       {gap:.4f}\\n\\n\")\n",
        "    f.write(f\"2. TEST METRICS\\n\")\n",
        "    f.write(f\"Precision: {prec:.4f}\\nRecall:    {rec:.4f}\\nF1-Score:  {f1_test:.4f}\\n\\n\")\n",
        "    f.write(f\"3. CLASSIFICATION REPORT\\n{report}\\n\")\n",
        "\n",
        "# Confusion Matrix Plot\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Pred: Safe', 'Pred: Hazard'], yticklabels=['True: Safe', 'True: Hazard'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig(os.path.join(map_dir, 'Stat_Confusion_Matrix.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# --- TEIL C: FEATURE IMPORTANCE ---\n",
        "print(\"\\n--- C. FEATURE IMPORTANCE ---\")\n",
        "imp_df = pd.DataFrame({'Feature': features, 'Importance': rf.feature_importances_}).sort_values('Importance', ascending=False)\n",
        "imp_df.to_csv(os.path.join(map_dir, 'Stat_Feature_Importance.csv'), index=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=imp_df, palette='viridis')\n",
        "plt.title('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(map_dir, 'Stat_Feature_Importance_Plot.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(f\"\\nFERTIG! Alle Statistiken (Bilder & Text) sind gespeichert in:\\n{map_dir}\")"
      ],
      "metadata": {
        "id": "1qrkjJU6k6xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import os\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Municipalities_polygon.shp'\n",
        "\n",
        "# Szenarien-Namen\n",
        "scenarios = ['SSP126', 'SSP370', 'SSP585']\n",
        "\n",
        "# 0=Grün, 0.5=Gelb, 1=Rot\n",
        "# ForestGreen (#228B22) -> Yellow (#FFFF00) -> Red (#FF0000)\n",
        "colors = [\"#228B22\", \"#FFFF00\", \"#FF0000\"]\n",
        "custom_cmap = LinearSegmentedColormap.from_list(\"GreenYellowRed\", colors)\n",
        "\n",
        "# DATEN LADEN\n",
        "print(\"Lade Shapefile für den Zoom...\")\n",
        "if os.path.exists(shapefile_path):\n",
        "    gdf_gem = gpd.read_file(shapefile_path)\n",
        "    if gdf_gem.crs != \"EPSG:4326\": gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "\n",
        "    minx, miny, maxx, maxy = gdf_gem.total_bounds\n",
        "    # Puffer (0.05 Grad ~ 5km), damit es nicht am Rand klebt\n",
        "    zoom_xlim = (minx - 0.05, maxx + 0.05)\n",
        "    zoom_ylim = (miny - 0.05, maxy + 0.05)\n",
        "    print(f\"Zoom gesetzt auf: Longitude {zoom_xlim}, Latitude {zoom_ylim}\")\n",
        "else:\n",
        "    print(\"Shapefile nicht gefunden!\")\n",
        "    # Fallback\n",
        "    zoom_xlim = (10.3, 12.5)\n",
        "    zoom_ylim = (46.2, 47.1)\n",
        "\n",
        "\n",
        "# KARTEN NEU PLOTTEN\n",
        "for name in scenarios:\n",
        "    csv_path = os.path.join(map_dir, f'Result_Grid_{name}.csv')\n",
        "\n",
        "    if os.path.exists(csv_path):\n",
        "        print(f\"Erstelle optimierte Karte für {name}...\")\n",
        "        df_scen = pd.read_csv(csv_path)\n",
        "\n",
        "        # Plotting Setup\n",
        "        fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "        # 1. Pixel-Map (Grün -> Gelb -> Rot)\n",
        "        # (Nur Punkte innerhalb des Zooms plotten)\n",
        "        mask = (df_scen.Longitude >= zoom_xlim[0]) & (df_scen.Longitude <= zoom_xlim[1]) & \\\n",
        "               (df_scen.Latitude >= zoom_ylim[0]) & (df_scen.Latitude <= zoom_ylim[1])\n",
        "        df_plot = df_scen[mask]\n",
        "\n",
        "        sc = ax.scatter(df_plot.Longitude, df_plot.Latitude,\n",
        "                        c=df_plot.Landslide_Probability,\n",
        "                        cmap=custom_cmap,\n",
        "                        vmin=0, vmax=1,\n",
        "                        s=15, marker='s', zorder=1, alpha=0.70)\n",
        "\n",
        "        # 2. Shapefile Overlay\n",
        "        if gdf_gem is not None:\n",
        "            gdf_gem.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.4, alpha=0.9, zorder=2)\n",
        "\n",
        "        # 3. Den Zoom anwenden\n",
        "        ax.set_xlim(zoom_xlim)\n",
        "        ax.set_ylim(zoom_ylim)\n",
        "\n",
        "        plt.colorbar(sc, label='Probability (0=Safe, 1=High Risk)')\n",
        "        plt.title(f'Future Projection: {name} (2081-2100)')\n",
        "        plt.xlabel('Longitude')\n",
        "        plt.ylabel('Latitude')\n",
        "\n",
        "        out_file = os.path.join(map_dir, f'Map_{name}_Zoomed.png')\n",
        "        plt.savefig(out_file, dpi=300, bbox_inches='tight') # bbox_inches='tight' entfernt weiße Ränder außen\n",
        "        plt.close()\n",
        "        print(f\"Gespeichert: {out_file}\")\n",
        "\n",
        "# GEMEINDEK ARTE AUCH UPDATEN\n",
        "ssp3_table = os.path.join(map_dir, \"Municipality_SSP3_Table.csv\")\n",
        "if os.path.exists(ssp3_table) and gdf_gem is not None:\n",
        "    print(\"Erstelle optimierte Gemeindekarte...\")\n",
        "    df_gem_data = pd.read_csv(ssp3_table)\n",
        "\n",
        "    # Spaltenname finden (Join Key)\n",
        "    key_col = df_gem_data.columns[0]\n",
        "\n",
        "    # Merge\n",
        "    gdf_final = gdf_gem.merge(df_gem_data, on=key_col)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    gdf_final.plot(column='Mean_Hazard_SSP3', ax=ax,\n",
        "                   cmap=custom_cmap,\n",
        "                   legend=True, legend_kwds={'label': \"Mean Hazard\"},\n",
        "                   edgecolor='black', linewidth=0.3)\n",
        "\n",
        "    ax.set_xlim(zoom_xlim)\n",
        "    ax.set_ylim(zoom_ylim)\n",
        "    plt.title(\"Municipality Risk (SSP3)\")\n",
        "    plt.savefig(os.path.join(map_dir, \"Municipality_Risk_SSP3_Zoomed.png\"), dpi=300, bbox_inches='tight')\n",
        "    print(\"Gemeindekarte gespeichert.\")\n",
        "\n",
        "print(\"\\nFERTIG!\")"
      ],
      "metadata": {
        "id": "2QSj8A46n1Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import geopandas as gpd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "hist_csv_path = os.path.join(map_dir, 'Risk_Map_Data_2015_2024.csv')\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Municipalities_polygon.shp'\n",
        "ssp_scenarios = ['SSP126', 'SSP370', 'SSP585']\n",
        "\n",
        "#COLORMAP DEFINIEREN ---\n",
        "# Hex-Codes: #008000 (Green), #FFFF00 (Bright Yellow), #FF0000 (Red)\n",
        "colors = [\"#009900\", \"#FFFF00\", \"#FF0000\"]\n",
        "custom_cmap = mcolors.LinearSegmentedColormap.from_list(\"TrafficLight\", colors)\n",
        "\n",
        "# 2. ZOOM EINSTELLEN\n",
        "print(\"Loading Shapefile...\")\n",
        "if os.path.exists(shapefile_path):\n",
        "    gdf_gem = gpd.read_file(shapefile_path)\n",
        "    if gdf_gem.crs != \"EPSG:4326\": gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "\n",
        "    minx, miny, maxx, maxy = gdf_gem.total_bounds\n",
        "    zoom_xlim = (minx - 0.02, maxx + 0.02)\n",
        "    zoom_ylim = (miny - 0.02, maxy + 0.02)\n",
        "else:\n",
        "    print(\"ERROR: Shapefile missing.\")\n",
        "    exit()\n",
        "\n",
        "def plot_pixel_map(df, title_text, output_filename):\n",
        "    mask = (df.Longitude >= zoom_xlim[0]) & (df.Longitude <= zoom_xlim[1]) & \\\n",
        "           (df.Latitude >= zoom_ylim[0]) & (df.Latitude <= zoom_ylim[1])\n",
        "    df_plot = df[mask]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    # 'custom_cmap' STATT 'RdYlGn_r'\n",
        "    sc = ax.scatter(df_plot.Longitude, df_plot.Latitude,\n",
        "                    c=df_plot.Landslide_Probability,\n",
        "                    cmap=custom_cmap, vmin=0, vmax=1,\n",
        "                    s=15, marker='s', zorder=1, alpha=0.8)\n",
        "\n",
        "    gdf_gem.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.5, alpha=0.9, zorder=2)\n",
        "\n",
        "    ax.set_xlim(zoom_xlim)\n",
        "    ax.set_ylim(zoom_ylim)\n",
        "\n",
        "    cbar = plt.colorbar(sc)\n",
        "    cbar.set_label('Landslide Probability (0=Low, 1=High)', rotation=270, labelpad=15)\n",
        "\n",
        "    plt.title(title_text, fontsize=14)\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "\n",
        "    save_path = os.path.join(map_dir, output_filename)\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Saved: {save_path}\")\n",
        "\n",
        "\n",
        "# 1. SSP KARTEN UPDATEN\n",
        "print(\"\\n--- Updating SSP Maps (Bright Colors) ---\")\n",
        "for name in ssp_scenarios:\n",
        "    csv_file = os.path.join(map_dir, f'Result_Grid_{name}.csv')\n",
        "    if os.path.exists(csv_file):\n",
        "        plot_pixel_map(pd.read_csv(csv_file), f\"Future Projection: {name} (2081-2100)\", f\"Map_{name}_Final.png\")\n",
        "\n",
        "# 2. HISTORISCHE PIXEL-KARTE\n",
        "print(\"\\n--- Updating Historical Map (Bright Colors) ---\")\n",
        "if os.path.exists(hist_csv_path):\n",
        "    plot_pixel_map(pd.read_csv(hist_csv_path), \"Historical Susceptibility (2015-2024)\", \"Map_Historical_2015_2024_Final.png\")\n",
        "\n",
        "    # 3. HISTORISCHE GEMEINDE-KARTE\n",
        "    df_hist = pd.read_csv(hist_csv_path)\n",
        "    gdf_points = gpd.GeoDataFrame(df_hist, geometry=gpd.points_from_xy(df_hist.Longitude, df_hist.Latitude), crs=\"EPSG:4326\")\n",
        "    joined = gpd.sjoin(gdf_points, gdf_gem, how=\"inner\", predicate=\"within\")\n",
        "\n",
        "    name_col = 'NAME_DE' if 'NAME_DE' in joined.columns else joined.columns[0]\n",
        "    risk_gem = joined.groupby(name_col)['Landslide_Probability'].mean().reset_index()\n",
        "    gdf_final = gdf_gem.merge(risk_gem.rename(columns={'Landslide_Probability': 'Mean_Hazard'}), on=name_col)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    gdf_final.plot(column='Mean_Hazard', ax=ax,\n",
        "                   cmap=custom_cmap, alpha=0.85,\n",
        "                   legend=True, legend_kwds={'label': \"Mean Susceptibility Index (0-1)\"},\n",
        "                   edgecolor='black', linewidth=0.3)\n",
        "\n",
        "    ax.set_xlim(zoom_xlim)\n",
        "    ax.set_ylim(zoom_ylim)\n",
        "    plt.title(\"Municipality Risk Assessment (2015-2024)\", fontsize=14)\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "    plt.savefig(os.path.join(map_dir, \"Municipality_Risk_2015_2024_Final.png\"), dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved Municipality Map.\")\n",
        "\n",
        "print(\"\\nFERTIG!\")"
      ],
      "metadata": {
        "id": "lQaqjVLipXN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "\n",
        "# Dateinamen definieren\n",
        "files = {\n",
        "    'Historical (2015-2024)': 'Risk_Map_Data_2015_2024.csv',\n",
        "    'SSP1-2.6 (2081-2100)': 'Result_Grid_SSP126.csv',\n",
        "    'SSP3-7.0 (2081-2100)': 'Result_Grid_SSP370.csv',\n",
        "    'SSP5-8.5 (2081-2100)': 'Result_Grid_SSP585.csv'\n",
        "}\n",
        "\n",
        "stats_list = []\n",
        "df_boxplot_list = [] # Für die Verteilungs-Grafik\n",
        "\n",
        "print(\"Berechne Statistiken...\")\n",
        "\n",
        "# 2. LOOP DURCH ALLE SZENARIEN\n",
        "for label, filename in files.items():\n",
        "    path = os.path.join(map_dir, filename)\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "        # Metriken berechnen\n",
        "        mean_risk = df['Landslide_Probability'].mean()\n",
        "        median_risk = df['Landslide_Probability'].median()\n",
        "\n",
        "        # \"High Risk Area\": Wieviel % der Punkte sind > 0.5 (Threshold)?\n",
        "        high_risk_pixels = df[df['Landslide_Probability'] > 0.5].shape[0]\n",
        "        total_pixels = df.shape[0]\n",
        "        high_risk_percent = (high_risk_pixels / total_pixels) * 100\n",
        "\n",
        "        # Speichern für Tabelle\n",
        "        stats_list.append({\n",
        "            'Scenario': label,\n",
        "            'Mean_Risk_Index': mean_risk,\n",
        "            'High_Risk_Area_Percent': high_risk_percent,\n",
        "            'Pixel_Count_High_Risk': high_risk_pixels\n",
        "        })\n",
        "\n",
        "        # Speichern für Boxplot (nur Wahrscheinlichkeiten und Name)\n",
        "        temp_df = pd.DataFrame({\n",
        "            'Probability': df['Landslide_Probability'],\n",
        "            'Scenario': label\n",
        "        })\n",
        "        df_boxplot_list.append(temp_df)\n",
        "\n",
        "    else:\n",
        "        print(f\"WARNUNG: Datei {filename} nicht gefunden.\")\n",
        "\n",
        "# 3. ZUSAMMENFÜHREN\n",
        "stats_df = pd.DataFrame(stats_list)\n",
        "\n",
        "# Berechnung der Veränderung (Delta) relativ zu Historical\n",
        "hist_row = stats_df[stats_df['Scenario'] == 'Historical (2015-2024)']\n",
        "if not hist_row.empty:\n",
        "    hist_mean = hist_row['Mean_Risk_Index'].values[0]\n",
        "    hist_area = hist_row['High_Risk_Area_Percent'].values[0]\n",
        "\n",
        "    # Neue Spalten: Delta absolute und relative\n",
        "    stats_df['Change_Mean_Risk (%)'] = ((stats_df['Mean_Risk_Index'] - hist_mean) / hist_mean) * 100\n",
        "    stats_df['Change_High_Risk_Area (%)'] = ((stats_df['High_Risk_Area_Percent'] - hist_area) / hist_area) * 100\n",
        "\n",
        "    # Bei Historical selbst Nullen setzen\n",
        "    stats_df.loc[stats_df['Scenario'] == 'Historical (2015-2024)', 'Change_Mean_Risk (%)'] = 0\n",
        "    stats_df.loc[stats_df['Scenario'] == 'Historical (2015-2024)', 'Change_High_Risk_Area (%)'] = 0\n",
        "\n",
        "\n",
        "# 4. AUSGABE & SPEICHERN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RISIKO-VERGLEICH TABELLE\")\n",
        "print(\"=\"*60)\n",
        "print(stats_df[['Scenario', 'Mean_Risk_Index', 'High_Risk_Area_Percent', 'Change_High_Risk_Area (%)']].to_string(index=False))\n",
        "\n",
        "# CSV speichern\n",
        "out_csv = os.path.join(map_dir, 'Final_Scenario_Statistics.csv')\n",
        "stats_df.to_csv(out_csv, index=False)\n",
        "print(f\"\\n-> Statistik gespeichert: {out_csv}\")\n",
        "\n",
        "\n",
        "# 5. VISUALISIERUNG\n",
        "\n",
        "# A) Balkendiagramm: High Risk Area\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['gray', 'green', 'orange', 'red'] # Passend zu Hist, SSP1, SSP3, SSP5\n",
        "sns.barplot(data=stats_df, x='Scenario', y='High_Risk_Area_Percent', palette=colors)\n",
        "plt.ylabel('Share of Area with High Risk (>50%)')\n",
        "plt.title('Expansion of High-Risk Zones across Scenarios')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.savefig(os.path.join(map_dir, 'Stat_High_Risk_Comparison.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# B) Boxplot: Verteilung der Risiken (Zeigt Unsicherheit/Streuung)\n",
        "# Hier sieht man, ob sich das ganze Risiko verschiebt oder nur die Extreme\n",
        "full_dist_df = pd.concat(df_boxplot_list)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.violinplot(data=full_dist_df, x='Scenario', y='Probability', palette=colors, inner='quartile')\n",
        "plt.ylabel('Predicted Landslide Probability')\n",
        "plt.title('Risk Distribution Density by Scenario')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.savefig(os.path.join(map_dir, 'Stat_Risk_Distribution.png'), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "print(\"-> Grafiken gespeichert.\")"
      ],
      "metadata": {
        "id": "4eEQTWCms018"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import geopandas as gpd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. SETUP\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Landslides'\n",
        "data_dir = os.path.join(base_dir, 'Data')\n",
        "map_dir = os.path.join(base_dir, 'Output_Maps')\n",
        "\n",
        "# PFADE DEFINIEREN\n",
        "# Die Hintergrund-Karte (Pixel-Daten 2015-2024)\n",
        "hist_grid_path = os.path.join(map_dir, 'Risk_Map_Data_2015_2024.csv')\n",
        "# Trainingsdaten\n",
        "train_data_path = os.path.join(data_dir, 'landslides.csv')\n",
        "# Gemeindegrenzen\n",
        "shapefile_path = '/content/drive/MyDrive/Landslides/Data/FME_11060556_1767623643023_2240/Municipalities_polygon.shp'\n",
        "\n",
        "# CUSTOM COLORMAP\n",
        "colors = [\"#009900\", \"#FFFF00\", \"#FF0000\"]\n",
        "custom_cmap = mcolors.LinearSegmentedColormap.from_list(\"TrafficLight\", colors)\n",
        "\n",
        "# 2. DATEN LADEN & ZOOM BERECHNEN\n",
        "print(\"Lade Shapefile & berechne Zoom...\")\n",
        "if os.path.exists(shapefile_path):\n",
        "    gdf_gem = gpd.read_file(shapefile_path)\n",
        "    if gdf_gem.crs != \"EPSG:4326\": gdf_gem = gdf_gem.to_crs(\"EPSG:4326\")\n",
        "    minx, miny, maxx, maxy = gdf_gem.total_bounds\n",
        "    zoom_xlim = (minx - 0.02, maxx + 0.02)\n",
        "    zoom_ylim = (miny - 0.02, maxy + 0.02)\n",
        "else:\n",
        "    print(\"ERROR: Shapefile not found.\")\n",
        "    exit()\n",
        "\n",
        "print(\"Lade Hintergrund-Rasterdaten (Historical)...\")\n",
        "if not os.path.exists(hist_grid_path):\n",
        "    print(f\"ERROR: {hist_grid_path} nicht gefunden\")\n",
        "    exit()\n",
        "df_hist_grid = pd.read_csv(hist_grid_path)\n",
        "\n",
        "# ECHTE EVENTS LADEN & FILTERN\n",
        "print(\"Lade echte Erdrutsch-Events...\")\n",
        "if not os.path.exists(train_data_path):\n",
        "    print(f\"ERROR: Trainingsdaten {train_data_path} nicht gefunden.\")\n",
        "    exit()\n",
        "\n",
        "df_train = pd.read_csv(train_data_path)\n",
        "\n",
        "# 1. Nur echte Rutschungen (Target = 1)\n",
        "real_events = df_train[df_train['Target'] == 1].copy()\n",
        "\n",
        "# 2. Nach Zeitraum 2015-2024 filtern (damit es zur Karte passt)\n",
        "date_col = None\n",
        "for c in real_events.columns:\n",
        "    if 'date' in c.lower() or 'jahr' in c.lower() or 'year' in c.lower():\n",
        "        date_col = c\n",
        "        break\n",
        "\n",
        "if date_col:\n",
        "    print(f\"Filtere Events für Zeitraum 2015-2024 (basierend auf Spalte '{date_col}')...\")\n",
        "    # Versuche Datum zu parsen und Jahr zu extrahieren\n",
        "    real_events['Year_Extracted'] = pd.to_datetime(real_events[date_col], errors='coerce').dt.year\n",
        "    # Filtern\n",
        "    events_filtered = real_events[(real_events['Year_Extracted'] >= 2015) & (real_events['Year_Extracted'] <= 2024)]\n",
        "    print(f\"-> {len(events_filtered)} Events im Zeitraum gefunden (von total {len(real_events)}).\")\n",
        "else:\n",
        "    print(\"WARNUNG: Keine Datumsspalte gefunden. Zeige alle historischen Events aus den Trainingsdaten.\")\n",
        "    events_filtered = real_events\n",
        "\n",
        "# PLOTTING\n",
        "print(\"Erstelle Karte mit Overlay...\")\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# 1. HINTERGRUND: Die Risikokarte (Pixel)\n",
        "# Filtern der Pixel auf den Zoom-Bereich für schnelleres Plotten\n",
        "mask_grid = (df_hist_grid.Longitude >= zoom_xlim[0]) & (df_hist_grid.Longitude <= zoom_xlim[1]) & \\\n",
        "            (df_hist_grid.Latitude >= zoom_ylim[0]) & (df_hist_grid.Latitude <= zoom_ylim[1])\n",
        "df_plot_grid = df_hist_grid[mask_grid]\n",
        "\n",
        "sc = ax.scatter(df_plot_grid.Longitude, df_plot_grid.Latitude,\n",
        "                c=df_plot_grid.Landslide_Probability,\n",
        "                cmap=custom_cmap, vmin=0, vmax=1,\n",
        "                s=15, marker='s', zorder=1, alpha=0.8)\n",
        "\n",
        "# 2. EBENE: Gemeindegrenzen\n",
        "gdf_gem.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.5, alpha=0.9, zorder=2)\n",
        "\n",
        "# 3. VORDERGRUND: Die echten Events (Schwarze Kreuze)\n",
        "# Zorder=3 sorgt dafür, dass sie ganz oben liegen\n",
        "ax.scatter(events_filtered.Longitude, events_filtered.Latitude,\n",
        "           c='black', marker='x', s=80, linewidth=1.5,\n",
        "           label=f'Observed Landslides ({len(events_filtered)} events)', zorder=3)\n",
        "\n",
        "# Styling\n",
        "ax.set_xlim(zoom_xlim)\n",
        "ax.set_ylim(zoom_ylim)\n",
        "\n",
        "cbar = plt.colorbar(sc)\n",
        "cbar.set_label('Modelled Probability (0=Low, 1=High)', rotation=270, labelpad=15)\n",
        "\n",
        "plt.title(\"Model Validation: Predicted Risk vs. Observed Events (2015-2024)\", fontsize=14)\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.legend(loc='upper right', frameon=True, facecolor='white', framealpha=0.9)\n",
        "\n",
        "# Speichern mit neuem Namen\n",
        "out_file = os.path.join(map_dir, 'Map_Historical_2015_2024_Validation_Overlay.png')\n",
        "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"\\nFERTIG! Validierungskarte gespeichert unter:\\n{out_file}\")"
      ],
      "metadata": {
        "id": "NWdPr-De2Ckh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
